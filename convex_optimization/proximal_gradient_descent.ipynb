{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Gradient Descent Takeaway\n",
    "\n",
    "### 解决什么问题\n",
    "\n",
    "$$\n",
    "\\min_{x} f(x)=\\min_{x}\\left\\{g(x)+h(x)\\right\\}\n",
    "$$\n",
    "\n",
    "$g$ convex，differentiable\n",
    "$h$ convex，not differentiable，一般作为限制函数。\n",
    "\n",
    "### Proximal Mapping\n",
    "\n",
    "$$\n",
    "Prox_{h,t}(x)=\\argmin_z \\frac{1}{2t} \\lVert x-z\\rVert _2^2 + h(z)\n",
    "$$\n",
    "即给定一个$x$，找到最优点$z=prox_{h,t}(x)$，使得$\\frac{1}{2t} \\lVert x-z\\rVert _2^2 + h(z)$最小，这样的$z$能够使得$h$足够小，而且接近不可微点$x$。\n",
    "\n",
    "### Proximal Gradient Descent\n",
    "\n",
    "$$\n",
    "x_{k+1}=Prox_{h,t_k}(x_k-t_k\\nabla g(x_k))\n",
    "$$\n",
    "即给定起点$x_k$，首先沿着$g$的负梯度方向更新一个值$x_k-t_k\\nabla g(x_k)$，然后用近端映射寻找一个$z$，这个$z$能使得不可微函数$h$足够小，而且接近这个$x_k-t_k\\nabla g(x_k)$，就用这个$z$作为本次迭代的更新值。\n",
    "\n",
    "# QA\n",
    "\n",
    "### 和Gradient Descent什么关系\n",
    "\n",
    "常规梯度下降：$h(x)=0$。\n",
    "\n",
    "近端映射$prox_{t}(x)=\\argmin_{z} \\frac{1}{2t}\\lVert x-z\\rVert _2^2 + 0=x$\n",
    "\n",
    "这个时候的更新策略：$x_{k+1}=x_{k}-t_k\\nabla g(x_k)$\n",
    "\n",
    "# 和Projection Descent什么关系\n",
    "\n",
    "投影梯度下降：$h(x)=I_C(x)$\n",
    "\n",
    "近端映射：$prox_{t}=\\argmin_{z}\\frac{1}{2t}\\lVert x-z\\rVert _2^2+I_C(z)=\\argmin_{z\\in C}\\frac{1}{2t}\\lVert x-z\\rVert _2^2=\\pi_{C}(x)$\n",
    "\n",
    "更新策略：$x_{k+1}=\\pi_C(x_k-t_k \\nabla g(x_k))$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
